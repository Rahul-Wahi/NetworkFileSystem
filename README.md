# NetworkFileSystem
**Introduction:** This project is about implementing network file system (NFS). NFS allows clients to access data over network much like local storage is accessed.
**Problem Statement:** As the data is coming over network, accessing data from server is expensive and if the server is down or any error occurs at server then client won’t get the correct data. Also, as the multiple clients are access the same data from server it can overload a single server. Some efficient way of data access, error detection and error recovery mechanism is needed for NFS
**Goal:** Goal of this project is to distribute data across multiple data servers to
1.	To reduce load on servers
1. 	Provide increase aggregate capacity
1.	Increase fault tolerant

**Design & Implementation:** To reduce the load, distribute the blocks evenly across multiple servers. So large file access can happen parallelly. All the layers above disk block (Inode, File etc) will have the same implementation as of single server, only difference is now that the block number used by them are virtual instead of physical block number of any server. So, on client side we need to map this virtual block numbers to {sever, physical block number}. So, Client side Get/Put will first map the virtual block number then call the Get/Put on respective server and block number. Below is distribution of virtual block number across 5 severs.

**Capacity Aggregation:** Also, the capacity of system will be increased. If N is total number of servers, then total capacity of the system is (N - 1) times the single server capacity (C) approx. As client are using virtual block numbers, so this number now will range from 0 to ((N-1) * C) –  1.

**Fault Tolerance:** For fault tolerance, we will use the parity information. For each N - 1 blocks, we will use 1 block to store their parity information. Parity is calculated by XORing the content of N – 1 block. On every write call on block, client will calculate the parity and update the parity block. This parity is also distributed across servers to avoid the bottleneck of multiple writes of parity information. This parity information will help recover from single disk failure or block corruption. Also, we will be using extra storage on disk for storing the checksum of each block. On write call to this block server recalculates the checksum and store it. On read server calculate the checksum of data and verify it with the stored checksum. So, if checksum do not match server returns error to the client. So system can recover from 2 types of errors (occurring single type of error at a time).
1.	If a single server is down, client can get the data by XORing the content of other disk blocks including parity block.
1.	If sever is up but block is corrupted, server will detect the error using checksum and return the error to client, client can get the data by XORing the content of other disk blocks including parity block.

**Conclusions:** Storing data on the server and accessing this data over network much like local storage can poses various challenges like loss of data, data corruption, slow read/write, heavy load on single server. Having redundant storage can provide a robust way to deal with problems. In this project we evaluated how distributing data across multiple servers and having a distributed redundant storage can help in deal with data loss/ corruption and load distribution. We evaluated how in RAID-5 architecture with expensive Put calls ( but still load on single server is less than as compared to Single server Put calls), all the GET calls are almost evenly distributed across the servers and also how its enables the data recovery in case of server crash or data corruption.
